{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from torch.optim import Adam\n",
    "plt.ion()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = 0\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(CUDA_DEVICE)\n",
    "\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'Train' : tv.transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        #transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'Val' : tv.transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'Test' : tv.transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../Project/dataset\"\n",
    "datasets = ['Train','Val','Test']\n",
    "image_data = {}\n",
    "for x in datasets:\n",
    "    image_data[x] = tv.datasets.ImageFolder(data_folder+'/'+x, transform=data_transform[x])\n",
    "    \n",
    "dataset_sizes = {x: len(image_data[x]) for x in datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = {}\n",
    "for x in datasets:\n",
    "    data_loader[x] = torch.utils.data.DataLoader(image_data[x], batch_size=32,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "class_names = image_data['Train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_sizes = {'Train' : 2900, 'Val' : 725, 'Test' : 28}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0881 Acc: 0.2352\n",
      "trying epoch loss\n",
      "Val Loss: 0.0698 Acc: 0.4538\n",
      "new best accuracy =  0.45379310344827584\n",
      "Epoch 1/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0749 Acc: 0.3721\n",
      "trying epoch loss\n",
      "Val Loss: 0.0587 Acc: 0.5848\n",
      "new best accuracy =  0.5848275862068966\n",
      "Epoch 2/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0679 Acc: 0.4483\n",
      "trying epoch loss\n",
      "Val Loss: 0.0525 Acc: 0.6193\n",
      "new best accuracy =  0.6193103448275862\n",
      "Epoch 3/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0652 Acc: 0.4731\n",
      "trying epoch loss\n",
      "Val Loss: 0.0481 Acc: 0.6428\n",
      "new best accuracy =  0.6427586206896552\n",
      "Epoch 4/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0604 Acc: 0.5076\n",
      "trying epoch loss\n",
      "Val Loss: 0.0441 Acc: 0.7021\n",
      "new best accuracy =  0.7020689655172414\n",
      "Epoch 5/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0589 Acc: 0.5128\n",
      "trying epoch loss\n",
      "Val Loss: 0.0419 Acc: 0.7172\n",
      "new best accuracy =  0.7172413793103448\n",
      "Epoch 6/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0564 Acc: 0.5469\n",
      "trying epoch loss\n",
      "Val Loss: 0.0388 Acc: 0.7434\n",
      "new best accuracy =  0.743448275862069\n",
      "Epoch 7/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0534 Acc: 0.5707\n",
      "trying epoch loss\n",
      "Val Loss: 0.0363 Acc: 0.7655\n",
      "new best accuracy =  0.7655172413793103\n",
      "Epoch 8/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0540 Acc: 0.5476\n",
      "trying epoch loss\n",
      "Val Loss: 0.0343 Acc: 0.7738\n",
      "new best accuracy =  0.7737931034482759\n",
      "Epoch 9/9\n",
      "----------\n",
      "trying epoch loss\n",
      "Train Loss: 0.0514 Acc: 0.5676\n",
      "trying epoch loss\n",
      "Val Loss: 0.0335 Acc: 0.7628\n",
      "Training complete in 19m 20s\n",
      "Best val Acc: 0.773793\n",
      "returning and looping back\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "    store_train_loss = []\n",
    "    store_train_acc = []\n",
    "    store_val_loss = []\n",
    "    store_val_acc = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['Train', 'Val']:\n",
    "            if phase == 'train':\n",
    "                mode='train'\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "                mode='val'\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            counter=0\n",
    "            # Iterate over data.\n",
    "            for data in data_loader[phase]:\n",
    "                inputs, labels = data\n",
    "                #print(inputs.size())\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    try:\n",
    "                        inputs, labels = Variable(inputs.float().cuda()),                             \n",
    "                        Variable(labels.long().cuda())\n",
    "                    except:\n",
    "                        print(inputs,labels)\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                # print('loss done')                \n",
    "                # Just so that you can keep track that something's happening and don't feel like the program isn't running.\n",
    "                # if counter%10==0:\n",
    "                #     print(\"Reached iteration \",counter)\n",
    "                counter+=1\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'Train':\n",
    "                    # print('loss backward')\n",
    "                    loss.backward()\n",
    "                    # print('done loss backward')\n",
    "                    optimizer.step()\n",
    "                    # print('done optim')\n",
    "                # print evaluation statistics\n",
    "                try:\n",
    "                    # running_loss += loss.data[0]\n",
    "                    running_loss += loss.item()\n",
    "                    # print(labels.data)\n",
    "                    # print(preds)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    # print('running correct =',running_corrects)\n",
    "                except:\n",
    "                    print('unexpected error, could not calculate loss or do a sum.')\n",
    "            print('trying epoch loss')\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects.item() / float(dset_sizes[phase])\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            if phase == 'Train':\n",
    "                store_train_loss.append(epoch_loss)\n",
    "                store_train_acc.append(epoch_acc)\n",
    "            elif phase == 'Val':\n",
    "                store_val_loss.append(epoch_loss)\n",
    "                store_val_acc.append(epoch_acc)\n",
    "\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'Val':\n",
    "                #if USE_TENSORBOARD:\n",
    "                #    foo.add_scalar_value('epoch_loss',epoch_loss,step=epoch)\n",
    "                #    foo.add_scalar_value('epoch_acc',epoch_acc,step=epoch)\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    print('new best accuracy = ',best_acc)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('returning and looping back')\n",
    "    return best_model\n",
    "\n",
    "# This function changes the learning rate over the training model.\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.01, lr_decay_epoch=1):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (1**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "model_ft = models.alexnet(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_features = model_ft.classifier[6].in_features\n",
    "features = list(model_ft.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\n",
    "model_ft.classifier = nn.Sequential(*features) \n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_gpu:\n",
    "    criterion.cuda()\n",
    "    model_ft.cuda()\n",
    "\n",
    "optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=10)\n",
    "\n",
    "# Save model\n",
    "#model_ft.state_dict('fine_tuned_best_model.pt')\n",
    "#model_ft.state_dict('mytraining.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.eval()\n",
    "torch.save(model_ft.state_dict(), 'mytraining3.pt') \n",
    "#print(model_ft.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(m):\n",
    "    print(type(m))\n",
    "    test_acc = 0.0\n",
    "    for i, (images, labels) in enumerate(data_loader['Test']):\n",
    "\n",
    "#         if cuda_avail:\n",
    "#             images = Variable(images.cuda())\n",
    "#             labels = Variable(labels.cuda())\n",
    "\n",
    "        # Predict classes using images from the test set\n",
    "        outputs = m(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        test_acc += torch.sum(prediction == labels.data)\n",
    "\n",
    "    # Compute the average acc and loss over all 10000 test images\n",
    "    test_acc = test_acc.item() / dset_sizes['Test']\n",
    "    print(dset_sizes['Train'],dset_sizes['Val'],dset_sizes['Test'])\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.models.alexnet.AlexNet'>\n",
      "2900 725 28\n",
      "0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "model = models.alexnet(pretrained=True)\n",
    "num_features = model.classifier[6].in_features\n",
    "features = list(model.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\n",
    "model.classifier = nn.Sequential(*features) \n",
    "model.load_state_dict(torch.load('mytraining_hf_vf.pt'))\n",
    "model.eval()\n",
    "acc = test(model)\n",
    "print((acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'store_train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-428e9a36ab8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'store_train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(store_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
